{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20d1f98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 988\n",
      "預測樣本數： 247\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_append(data,n):\n",
    "    newdf = pd.DataFrame(np.repeat(data.values,n,axis=0))\n",
    "    newdf.columns = df.columns\n",
    "    return newdf\n",
    "\n",
    "df = pd.read_csv(\"512_all.csv\")\n",
    "df = df.dropna(axis = 0,how = 'any')\n",
    "\n",
    "df_train,df_test = train_test_split(df, test_size=0.2 ,random_state=10)\n",
    "\n",
    "df_test = df_test.drop('label',axis=1)\n",
    "id_test = [i for i in range(len(df_test))]\n",
    "df_test.loc[:,'id'] = id_test\n",
    "\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"訓練樣本數：\", len(df_train))\n",
    "\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"預測樣本數：\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6714dc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>成為優秀成功領導人,帶領團隊更上巔峰、創造績效。新人茶會：針對新進人員介紹公司文化與現狀,了...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>推廣客製化服務,達到個人化精準行銷。深耕數位平台客戶,提升外部獲客能力數位化主要策略是以客戶...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>為全國唯一具有HCE與掃碼付功能的手機信用卡。在智能金融方面,以大數據及數位平台發展為核心,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>並辦理盈餘轉增資42億元,發放前(1年度股票股利每股50元及現金股利每股10元。在公司治理方...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>﻿107年度營業計畫概要經營方針106年全球經濟穩健擴張,美國於去年3度宣布升息,並持續升息...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>以因應金融科技之發展。充分投資收益性及安全性較佳之金融商品,加強餘裕資金之管理,以賺取利息收...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>以確保本集團之法令遵循制度及作業符合國內外金融監理之潮流與趨勢。強化共同行銷集團共銷效益：1...</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>並提供保戶完整的專業服務。長期持續致力於商品創新,華南產險在「最佳保險專業」及「最佳保險商品...</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>458萬元,稅前淨損為6億2,760萬元,所得稅利益為7億4,964萬元,稅後純益為1億2,...</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>關注並適時引進建置FinTech/InsurTech相關應用,增進經營效率及業務行銷力。結合...</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text_a   id\n",
       "123  成為優秀成功領導人,帶領團隊更上巔峰、創造績效。新人茶會：針對新進人員介紹公司文化與現狀,了...    0\n",
       "824  推廣客製化服務,達到個人化精準行銷。深耕數位平台客戶,提升外部獲客能力數位化主要策略是以客戶...    1\n",
       "874  為全國唯一具有HCE與掃碼付功能的手機信用卡。在智能金融方面,以大數據及數位平台發展為核心,...    2\n",
       "699  並辦理盈餘轉增資42億元,發放前(1年度股票股利每股50元及現金股利每股10元。在公司治理方...    3\n",
       "363  ﻿107年度營業計畫概要經營方針106年全球經濟穩健擴張,美國於去年3度宣布升息,並持續升息...    4\n",
       "..                                                 ...  ...\n",
       "786  以因應金融科技之發展。充分投資收益性及安全性較佳之金融商品,加強餘裕資金之管理,以賺取利息收...  242\n",
       "531  以確保本集團之法令遵循制度及作業符合國內外金融監理之潮流與趨勢。強化共同行銷集團共銷效益：1...  243\n",
       "399  並提供保戶完整的專業服務。長期持續致力於商品創新,華南產險在「最佳保險專業」及「最佳保險商品...  244\n",
       "20   458萬元,稅前淨損為6億2,760萬元,所得稅利益為7億4,964萬元,稅後純益為1億2,...  245\n",
       "39   關注並適時引進建置FinTech/InsurTech相關應用,增進經營效率及業務行銷力。結合...  246\n",
       "\n",
       "[247 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5339174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要驗證集\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'Y': 0, 'N': 1}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a = self.df.iloc[idx, :1].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a+ [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        return (tokens_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"schen/longformer-chinese-base-4096\")\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01b56bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][1] is not None:\n",
    "        label_ids = torch.stack([s[1] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f966a482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：9-7%健傷保險618,297699,552(81,2-6%財責保險1,472,5731,413,22659,347+2%運輸保險642,762690,092(47,3-9%合計7,333,8817,221,241112\n",
      "分類  ：N\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([  101,   130,   118,   128,   110,   978,  1003,   924,  7402,  9048,\n",
      "          117, 11992,  9398,  8160,   117,  8222,  8144,   113,  8424,   117,\n",
      "          123,   118,   127,   110,  6512,  6519,   924,  7402,   122,   117,\n",
      "         8264,  8144,   117,  8272,  8805,   117, 12561,   117, 10436,  9632,\n",
      "          117, 12936,   116,   123,   110,  6880,  6745,   924,  7402,  8308,\n",
      "         8144,   117,  8399,  8756,  8599,   117,  8141,  8144,   113,  8264,\n",
      "          117,   124,   118,   130,   110,  1394,  6243,   128,   117, 10745,\n",
      "          117,  8302,  8408,   117, 10118,   117, 10896,  8452,  8144,   102])\n",
      "\n",
      "label_tensor   ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "text_a, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text_a}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06979502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 407])\n",
      "torch.Size([64, 407])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 411])\n",
      "torch.Size([64, 411])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 446])\n",
      "torch.Size([64, 446])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 413])\n",
      "torch.Size([64, 413])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 432])\n",
      "torch.Size([64, 432])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 437])\n",
      "torch.Size([64, 437])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 432])\n",
      "torch.Size([64, 432])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 409])\n",
      "torch.Size([64, 409])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 399])\n",
      "torch.Size([64, 399])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 397])\n",
      "torch.Size([64, 397])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 447])\n",
      "torch.Size([64, 447])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 429])\n",
      "torch.Size([64, 429])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 431])\n",
      "torch.Size([64, 431])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 430])\n",
      "torch.Size([64, 430])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 445])\n",
      "torch.Size([64, 445])\n",
      "torch.Size([64])\n",
      "torch.Size([28, 422])\n",
      "torch.Size([28, 422])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "for i in iter(trainloader):\n",
    "    data = i\n",
    "    tokens_tensors,masks_tensors, label_ids = data\n",
    "    \n",
    "    print(tokens_tensors.shape)\n",
    "    print(masks_tensors.shape)\n",
    "    print(label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f722212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 407]) \n",
      "tensor([[ 101,  130,  118,  ...,    0,    0,    0],\n",
      "        [ 101, 8632, 2399,  ...,    0,    0,    0],\n",
      "        [ 101, 2990, 1285,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3300, 7302,  ...,    0,    0,    0],\n",
      "        [ 101, 3313,  889,  ...,    0,    0,    0],\n",
      "        [ 101, 4948, 3513,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 407])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors,masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e244eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification,AutoModel\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "# PRETRAINED_MODEL_NAME = \"schen/longformer-chinese-base-4096\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa76a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-chinese\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
    "\n",
    "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, masks_tensors = data[:2]\n",
    "            outputs = model(input_ids=tokens_tensors,attention_mask=masks_tensors)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[2]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 3  \n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors,masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors,  \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8df86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
