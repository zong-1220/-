{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabaafbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DC\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"fin_bert2000.csv\",error_bad_lines=False)\n",
    "df = df.drop(columns=['company', 'year','pena_n'])\n",
    "df = df.dropna(axis = 0,how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715d8ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>前一年度營業結果營業實施成果本公司於104年10月15日與中國信託金融控股股份有限公司(以下...</td>\n",
       "      <td>並提供保險顧問團隊輔訓服務深耕夥伴關係。直效行銷:持續增加電銷人員席次並提升產能。保險經紀人...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿前一年度營業結果105年度營業計畫實施成果105年,受新興市場成長動能放緩、美日歐表現不如...</td>\n",
       "      <td>強化財務結構降低營運及財務風險,提升營運績效1優化資產配置,佈局於收益率較佳之資產,提高經常...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿105年度營業結果回顧105年,受惠於國際電子產品需求增溫、原油及原物料價格止跌回穩,出口...</td>\n",
       "      <td>632件,成長率04%。保險給付為1,099億4,849萬元,較去年度增加88億1,053萬...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>特別為樂退經濟需求,量身訂作「展鑫人生利率變動型終身還本保險」,自選65歲三種樂退年齡,提供...</td>\n",
       "      <td>並藉由完整的選、用、育、留增員架構,全面提高增員動能及業務員定著率,藉以提升正式組長數；並積...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿前一年度營業結果105年度營業計畫實施成果本公司105年度經營績效報告如下：新契約保費收入...</td>\n",
       "      <td>以持續開拓利基商品及提供完整客戶服務。通路策略深耕外部通路並強化類策盟合作關係,鞏固市場地位...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>壹‧董事長劉炳輝先生106年度營業報告經濟局勢回顧106年,幾乎所有經濟調研機構在經濟報告上...</td>\n",
       "      <td>本行於106年度通過核准增設2處分行,分別設置於金融服務欠缺地區(高雄市燕巢區、苗栗縣銅鑼鄉...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>各位股東女士、先生,您好:在信用評等方面,惠譽信評於民國106年授予本行之評等結果為:國內長...</td>\n",
       "      <td>讓客戶享受金融科技帶來的便利。在企業金融業務方面,本行精準掌握產業脈動及市場需求,發揮集團遍...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>106年以來,全球經濟成長動能增強,先進、新興與開發中經濟體同步復甦,國際主要經濟預測機構多...</td>\n",
       "      <td>且政府積極推動前瞻基礎建設及振興經濟六大措施,包括公務員加薪、推動稅改、加速投資臺灣、法規鬆...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>前言民國106年,全球經濟同步復甦,景氣穩健擴張,本行兼顧資產安全、資金充裕與資本充足,穩健...</td>\n",
       "      <td>105億元。重要之經營政策基本政策:秉持誠信穩健經營,健康均衡永續發展,創造良好經營績效。業...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>各位股東女士、先生:總體經營環境受惠於全球景氣穩健復甦,國際原物料價格走高,台灣經濟表現在2...</td>\n",
       "      <td>獲得長期評等「A+」及短期評等「A-1」,評等展望為負向;以及中華信用評等公司授予長期發行體...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text_a  \\\n",
       "0    前一年度營業結果營業實施成果本公司於104年10月15日與中國信託金融控股股份有限公司(以下...   \n",
       "1    ﻿前一年度營業結果105年度營業計畫實施成果105年,受新興市場成長動能放緩、美日歐表現不如...   \n",
       "2    ﻿105年度營業結果回顧105年,受惠於國際電子產品需求增溫、原油及原物料價格止跌回穩,出口...   \n",
       "3    特別為樂退經濟需求,量身訂作「展鑫人生利率變動型終身還本保險」,自選65歲三種樂退年齡,提供...   \n",
       "4    ﻿前一年度營業結果105年度營業計畫實施成果本公司105年度經營績效報告如下：新契約保費收入...   \n",
       "..                                                 ...   \n",
       "175  壹‧董事長劉炳輝先生106年度營業報告經濟局勢回顧106年,幾乎所有經濟調研機構在經濟報告上...   \n",
       "176  各位股東女士、先生,您好:在信用評等方面,惠譽信評於民國106年授予本行之評等結果為:國內長...   \n",
       "177  106年以來,全球經濟成長動能增強,先進、新興與開發中經濟體同步復甦,國際主要經濟預測機構多...   \n",
       "178  前言民國106年,全球經濟同步復甦,景氣穩健擴張,本行兼顧資產安全、資金充裕與資本充足,穩健...   \n",
       "179  各位股東女士、先生:總體經營環境受惠於全球景氣穩健復甦,國際原物料價格走高,台灣經濟表現在2...   \n",
       "\n",
       "                                                text_b label  \n",
       "0    並提供保險顧問團隊輔訓服務深耕夥伴關係。直效行銷:持續增加電銷人員席次並提升產能。保險經紀人...     Y  \n",
       "1    強化財務結構降低營運及財務風險,提升營運績效1優化資產配置,佈局於收益率較佳之資產,提高經常...     Y  \n",
       "2    632件,成長率04%。保險給付為1,099億4,849萬元,較去年度增加88億1,053萬...     Y  \n",
       "3    並藉由完整的選、用、育、留增員架構,全面提高增員動能及業務員定著率,藉以提升正式組長數；並積...     Y  \n",
       "4    以持續開拓利基商品及提供完整客戶服務。通路策略深耕外部通路並強化類策盟合作關係,鞏固市場地位...     Y  \n",
       "..                                                 ...   ...  \n",
       "175  本行於106年度通過核准增設2處分行,分別設置於金融服務欠缺地區(高雄市燕巢區、苗栗縣銅鑼鄉...     N  \n",
       "176  讓客戶享受金融科技帶來的便利。在企業金融業務方面,本行精準掌握產業脈動及市場需求,發揮集團遍...     Y  \n",
       "177  且政府積極推動前瞻基礎建設及振興經濟六大措施,包括公務員加薪、推動稅改、加速投資臺灣、法規鬆...     N  \n",
       "178  105億元。重要之經營政策基本政策:秉持誠信穩健經營,健康均衡永續發展,創造良好經營績效。業...     Y  \n",
       "179  獲得長期評等「A+」及短期評等「A-1」,評等展望為負向;以及中華信用評等公司授予長期發行體...     N  \n",
       "\n",
       "[178 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3907e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 10\n",
      "預測樣本數： 3\n"
     ]
    }
   ],
   "source": [
    "df_train = df.head(10)\n",
    "df_test = df.tail(3)\n",
    "\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"訓練樣本數：\", len(df_train))\n",
    "\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"預測樣本數：\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ad629a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    0.55618\n",
       "Y    0.44382\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c40f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "from IPython.display import clear_output\n",
    "    \n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'Y': 0, 'N': 1}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "\n",
    "#PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "#tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"schen/longformer-chinese-base-4096\")\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d7f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1779])\n",
      "torch.Size([1502])\n",
      "torch.Size([1921])\n",
      "torch.Size([2366])\n",
      "torch.Size([1691])\n",
      "torch.Size([2482])\n",
      "torch.Size([2525])\n",
      "torch.Size([2101])\n",
      "torch.Size([1889])\n",
      "torch.Size([1299])\n"
     ]
    }
   ],
   "source": [
    "for i in iter(trainset):\n",
    "    data = i\n",
    "    tokens_tensor, segments_tensor, label_tensor = data\n",
    "    print(tokens_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8762f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 4\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ca67d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：前一年度營業結果營業實施成果本公司於104年10月15日與中國信託金融控股股份有限公司(以下簡稱中信金控)進行股份轉換,並成為中信金控持股百分之百之子公司。中信金控自100年11月接手大都會國際人壽(該公司於101年1月更名為中國信託人壽保險股份有限公司,以下簡稱中信人壽),中信人壽又於103年併購宏利人壽台灣分公司。為有效整合資源,以提升綜效,105年1月1日中信金控旗下兩家壽險子公司中信人壽與台灣人壽合併,資產已破兆,105年整體營運狀況良好,茲將經營績效報告如下:初年度保費為1,399億元,業界排名第4名,銀行保險、保險經紀人及代理人、直效行銷通路皆排名業界前三名,且業務部隊更加擴大,擁有超過九千名業務員。總保費為2,524億,業界排名第5名。可運用資金12,051億元。上年度預算執行情形:合併元年因進行整併,公司營運得以發揮綜效,預算執行情形良好,合併稅後淨利為49億元。財務收支及獲利能力分析合併稅後淨利‥34億元。合併資產總額:12,856億元。基本每股盈餘‥61元。研究發展狀況商品面傳統型商品:即時滿足通路及市場需求,搶佔既有市場,並因應市場及法令變化,快速推出商品,搶得先機。如:因應實物給付保單開放,105年11月為首家推出連結殯葬服務實物給付保單之壽險業者。投資型商品:除持續開發多檔臺外幣投資型商品及投資型商品內扣式附約保障平台,滿足消費者投資與保障的需求之外,亦積極開發附保證給付商品及連結新型態標的之投資型商品,如:105年12月推出附保證給付之商品。服務面:持續強化客戶及通路服務,提升服務品質與便利性,如:新增繳費管道。本年度營業概要經營方針於商品、通路、投資、服務面皆持續精進,銷售能帶來長期合理收益之商品,持續擴大公司規模,為公司創造長期價值。通路:持續發展多元通路,並致力於引進具價值之契約,以提升公司價值,各通路營運計畫如下:銀行保險:於銀行通路推動傳統型期繳及投資型商品銷售\n",
      "句子 2：並提供保險顧問團隊輔訓服務深耕夥伴關係。直效行銷:持續增加電銷人員席次並提升產能。保險經紀人及代理人:以核心保險經紀人及代理人營運模式增加通路黏著度。業務員:提升業務員產能並持續擴大業務員組織。企業保險:完善基礎建設,轉型為單一窗口多元產品線銷售與服務模式,推廣團體年金。商品:滿足客戶多元需求,並快速反應市場、通路需求及因應經濟環境及政策開發外幣及創新商品。投資:適當配置股票及債券,於穩定息收外獲取資本利得,提高公司收益。服務:運用資訊科技持續提升自動化及E化作業流程,以提升客戶及通路服務品質並提高作業效率增加客戶便利性。本年度營業目標106年度將以多元通路推廣高收益商品,並持續優化投報率,提升公司整體收益。未來公司發展策略通路銀行保險:以專業保險顧問深耕夥伴關係,擴大市場滲透率。直效行銷:透過細緻化經營提高成交率並擴大自有部隊席次,增加高收益商品銷售。業務員:透過深化客戶經營及強化招攬工具,提高產能並強化定著率。企業保險:複製成功經驗至其他通路。商品以客戶需求為導向發展商品銷售及服務,並聚焦高收益型商品,提高公司整體收益。投資於資產負債管理基礎下,優化投報率,並提升子公司獲利貢獻。服務提供差異化服務深耕客戶關係,並結合保險科技開創新商機,達成業務策略目標。受到外部競爭環境、法規環境及總體經營環境之影響主管機關擴大業務範圍:開放保險給付可採實物給付,給付樣態包含健康管理、醫療、長期照顧、護理、老人安養和殯葬,保戶可享多元化服務選擇;開放團體年金保險,且提供企業稅賦誘因,協助企業延攬並留住專業人才,同時替員工累積退休金;開放投資型附保證給付商品可採負債避險,有助控管商品風險及降低避險成本,此些政策皆可擴大業務範圍以提升成長動能,本公司亦積極投入新業務,適時抓住商機。主管機關放寬投資法令:開放投資國外地方政府發行或保證的債券及公共建設可派任董監事;放寬國外不動產投資額度;擬排除BBB+海外債券投資限額,上述政策將使保險業投資更加多元,有利提升投報率。主管機關管制類定存商品及鼓勵期繳商品銷售:主管機關近期不僅調降責任準備金利率,且嚴禁商品有費差損情形、分期繳費10年以上之商品,才100%計入商品評分獎勵,將使保險業者財務更加穩健。預計長期利率回升:考量民眾對利率回升的預期,今年商品將以利變型為主力,此外利率回升將有助壽險公司提升投報率並改善負利差情形,進而增加獲利。董事長\n",
      "分類  ：Y\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([ 101, 1184,  671,  ...,  752, 7269,  102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0,  ..., 1, 1, 1])\n",
      "\n",
      "label_tensor   ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]前一年度營業結果營業實施成果本公司於104年10月15日與中國信託金融控股股份有限公司(以下簡稱中信金控)進行股份轉換,並成為中信金控持股百分之百之子公司。中信金控自100年11月接手大都會國際人壽(該公司於101年1月更名為中國信託人壽保險股份有限公司,以下簡稱中信人壽),中信人壽又於103年併購宏利人壽台灣分公司。為有效整合資源,以提升綜效,105年1月1日中信金控旗下兩家壽險子公司中信人壽與台灣人壽合併,資產已破兆,105年整體營運狀況良好,茲將經營績效報告如下:初年度保費為1,399億元,業界排名第4名,銀行保險、保險經紀人及代理人、直效行銷通路皆排名業界前三名,且業務部隊更加擴大,擁有超過九千名業務員。總保費為2,52##4億,業界排名第5名。可運用資金12,05##1億元。上年度預算執行情形:合併元年因進行整併,公司營運得以發揮綜效,預算執行情形良好,合併稅後淨利為49億元。財務收支及獲利能力分析合併稅後淨利‥34億元。合併資產總額:12,85##6億元。基本每股盈餘‥61元。研究發展狀況商品面傳統型商品:即時滿足通路及市場需求,搶佔既有市場,並因應市場及法令變化,快速推出商品,搶得先機。如:因應實物給付保單開放,105年11月為首家推出連結殯葬服務實物給付保單之壽險業者。投資型商品:除持續開發多檔臺外幣投資型商品及投資型商品內扣式附約保障平台,滿足消費者投資與保障的需求之外,亦積極開發附保證給付商品及連結新型態標的之投資型商品,如:105年12月推出附保證給付之商品。服務面:持續強化客戶及通路服務,提升服務品質與便利性,如:新增繳費管道。本年度營業概要經營方針於商品、通路、投資、服務面皆持續精進,銷售能帶來長期合理收益之商品,持續擴大公司規模,為公司創造長期價值。通路:持續發展多元通路,並致力於引進具價值之契約,以提升公司價值,各通路營運計畫如下:銀行保險:於銀行通路推動傳統型期繳及投資型商品銷售[SEP]並提供保險顧問團隊輔訓服務深耕夥伴關係。直效行銷:持續增加電銷人員席次並提升產能。保險經紀人及代理人:以核心保險經紀人及代理人營運模式增加通路黏著度。業務員:提升業務員產能並持續擴大業務員組織。企業保險:完善基礎建設,轉型為單一窗口多元產品線銷售與服務模式,推廣團體年金。商品:滿足客戶多元需求,並快速反應市場、通路需求及因應經濟環境及政策開發外幣及創新商品。投資:適當配置股票及債券,於穩定息收外獲取資本利得,提高公司收益。服務:運用資訊科技持續提升自動化及e化作業流程,以提升客戶及通路服務品質並提高作業效率增加客戶便利性。本年度營業目標106年度將以多元通路推廣高收益商品,並持續優化投報率,提升公司整體收益。未來公司發展策略通路銀行保險:以專業保險顧問深耕夥伴關係,擴大市場滲透率。直效行銷:透過細緻化經營提高成交率並擴大自有部隊席次,增加高收益商品銷售。業務員:透過深化客戶經營及強化招攬工具,提高產能並強化定著率。企業保險:複製成功經驗至其他通路。商品以客戶需求為導向發展商品銷售及服務,並聚焦高收益型商品,提高公司整體收益。投資於資產負債管理基礎下,優化投報率,並提升子公司獲利貢獻。服務提供差異化服務深耕客戶關係,並結合保險科技開創新商機,達成業務策略目標。受到外部競爭環境、法規環境及總體經營環境之影響主管機關擴大業務範圍:開放保險給付可採實物給付,給付樣態包含健康管理、醫療、長期照顧、護理、老人安養和殯葬,保戶可享多元化服務選擇;開放團體年金保險,且提供企業稅賦誘因,協助企業延攬並留住專業人才,同時替員工累積退休金;開放投資型附保證給付商品可採負債避險,有助控管商品風險及降低避險成本,此些政策皆可擴大業務範圍以提升成長動能,本公司亦積極投入新業務,適時抓住商機。主管機關放寬投資法令:開放投資國外地方政府發行或保證的債券及公共建設可派任董監事;放寬國外不動產投資額度;擬排除bb##b+海外債券投資限額,上述政策將使保險業投資更加多元,有利提升投報率。主管機關管制類定存商品及鼓勵期繳商品銷售:主管機關近期不僅調降責任準備金利率,且嚴禁商品有費差損情形、分期繳費10年以上之商品,才100%計入商品評分獎勵,將使保險業者財務更加穩健。預計長期利率回升:考量民眾對利率回升的預期,今年商品將以利變型為主力,此外利率回升將有助壽險公司提升投報率並改善負利差情形,進而增加獲利。董事長[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text_a}\n",
    "句子 2：{text_b}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6710f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([4, 2366]) \n",
      "tensor([[ 101, 1184,  671,  ...,    0,    0,    0],\n",
      "        [ 101, 1184,  671,  ...,    0,    0,    0],\n",
      "        [ 101, 8423, 2399,  ...,    0,    0,    0],\n",
      "        [ 101, 4294, 1162,  ..., 6342, 2692,  102]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([4, 2366])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([4, 2366])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([4])\n",
      "tensor([0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae3e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2366])\n",
      "torch.Size([4, 2366])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 2525])\n",
      "torch.Size([4, 2525])\n",
      "torch.Size([4])\n",
      "torch.Size([2, 1889])\n",
      "torch.Size([2, 1889])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for i in iter(trainloader):\n",
    "    data = i\n",
    "    tokens_tensors, segments_tensors,masks_tensors, label_ids = data\n",
    "    \n",
    "    print(tokens_tensors.shape)\n",
    "    print(masks_tensors.shape)\n",
    "    print(label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24f0550c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification,AutoModel\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"schen/longformer-chinese-base-4096\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "#model = AutoModel.from_pretrained(\"schen/longformer-chinese-base-4096\")\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e3b8513",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"schen/longformer-chinese-base-4096\",\n",
       "  \"architectures\": [\n",
       "    \"BertForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512\n",
       "  ],\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9236b289",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "classification acc: 0.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
    "\n",
    "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 3  # 幸運數字\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d0290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
